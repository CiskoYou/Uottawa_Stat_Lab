---
title: "Lab-3: Régression linéaire multiple"
author: "Youssouph Cissokho"
date: "`r Sys.time()`"
fontsize: 11pt 
header-inlcude:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc_depth: true
    number_sections: true
urlcolor: blue
linkcolor: red
---

```{r setup, echo=FALSE}
library(formatR)
library(knitr)
options(width = 1000)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

`Table des matières`

Dans cette présentation, nous ferons ce qui suit 

* [Préliminaires](#11) ;
    a. [Voir la structure de l'ensemble de données](#12) ;
    b. [Visualisation de l'ensemble de données](#13) ;
* [Régression linéaire multiple](#1) ;
    a. [RL multiple avec le modèle complet](#2) ;
        i. [Signification des coefficients de la régression](#14)
        ii. [Signification du modèle](#15)
    b. [RL avec un prédicteur](#3) et [ici](#4)
* [Sélection du modèle](#5)
    a. [Basé sur la base de $R^2$, RSE et Mean$_{se}$ ajustés](#6)
    b. [Basé sur l'AIC](#7)
    c. [Basé sur Anova](#8)
* [Intervalles de confiance pour les paramètres](#9)
* [IC sur la réponse moyenne](#10)
* [Problème à faire](#11)


<br>
<br>
<br>
<br>
<br>
<br>

# Préliminaires {#11}
Un modèle de régression qui implique plus d'une variable de régression est appelé **modèle de régression multiple**. L'ajustement et l'analyse de ces modèles sont discutés dans ce laboratoire. Les résultats sont des extensions de ceux du `lab2` basé la régression linéaire simple.

Le modèle de `régression linéaire multiple avec k` régresseurs (prédicteurs) est donné par la formule suivante 
$$
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{k} x_{k}+\varepsilon
$$
où 

* $\beta_i,\;\;i=1\cdots,k$ sont les coefficients de régression ; 
* $\varepsilon$ est l'erreur aléatoire, c'est-à-dire
$$
\epsilon \sim N(0, \sigma^2).
$$
c'est-à-dire qu'il s'agit des variables aléatoires normales *indépendantes et identiquement distribuées* (iid) avec une moyenne de $0$ et une variance de $\sigma^2$.

Objectif : estimer les coefficients de régression.    $\beta_i,\;\;i=1\cdots,k$.

## Voir la structure du jeu de données {#12}

Nous allons utiliser le jeu de données `parenthood` du manuel **Learning statistics with R** de `Danielle Navarro`. La question est de savoir pourquoi Danielle est toujours de mauvaise humeur le lendemain?:

* Peut-être qu'elle ne dort pas assez.

Pour répondre à cette question, on va essayer d'examiner la relation entre la quantité de sommeil qu'elle reçoit et sa mauvaise humeur le jour suivant.

Examinons maintenant les `données` en détail : Les colonnes sont 

* `baby.sleep`: le niveau de sommeil du bébé;
* `dan.grump` : le degré de tristesse (dépression) de Danielle le jour suivant;
* `dan.sleep` : le niveau de sommeil de Danielle.

```{r data-exploration}
data_parent<-read.csv("parenthood.csv")   # charger la donnée parentalité 
head(data_parent, n=5)        # affiche les 5 lignes
str(data_parent)              # affiche la structure des elements
dim(data_parent)              # la dimension 
```

## Visualisation de l'ensemble des données {#13}

Le package `GGally` fournit une fonction nommée `ggpairs` qui est l'équivalent ggplot2 de la fonction pairs dans R.
```{r, message=FALSE}
# install.packages("GGally")
library(GGally)
ggpairs(data_parent, columns = 2:4) 
```

# Régression linéaire multiple {#1}

Nous allons effectuer 3 régressions différentes :

* En utilisant tous les prédicteurs (`baby.sleep` et `dan.sleep`) ;
* En utilisant un prédicteur à la fois (`baby.sleep` ou `dan.sleep`) ;
* Enfin, nous choisirons le **"meilleur modèle"**.

##  RlM avec le modèle complet {#2}

```{r }
# RLM avec tous les prédicteurs
Full_model = lm(dan.grump ~ dan.sleep + baby.sleep, data = data_parent) 
Full_model     # affiche les coefficients
```
Le modèle de régression linéaire multiple est donc le suivant

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep** + `r round(Full_model$coefficients[3],3)`* **baby.sleep**

Le coefficient associé à **dan.sleep** est assez important, ce qui suggère que chaque `heure de sommeil qu'elle perd cela augmente son niveau de dépression  (mauvaise humeur)`. Cependant, le coefficient associé à **baby.sleep** est très faible, ce qui suggère que le nombre d'heures de sommeil de son fils n'a pas **vraiment d'importance**. Ce qui compte, en ce qui concerne sa dépression, c'est le nombre d'heures de sommeil qu'elle a. 

Une fois les coefficients estimés, nous sommes confrontés à deux questions immédiates :

1. `Quelle est la performance globale du modèle` ? 
2. `Quels sont les régresseurs spécifiques (coefficients) qui semblent importants` ?

### La signification des coefficients de la régression {#14}

```{r }
# RLM avec tous les prédicteurs
Full_model = lm(dan.grump ~ dan.sleep + baby.sleep, data = data_parent) 
summary(Full_model)     # affiche coefficients
```

La troisième colonne donne la *statistique t* (chacune de la forme $t=\frac{\hat{\beta}}{se(\hat{\beta})}$), et sa *valeur p* réelle pour chacun de ces tests (dans la quatrième colonne) :

* Le coefficient de l'intercept ($\hat{\beta}_0=125.97$) et le coefficient de `dan.sleep` ($\hat{\beta}_1=-8.9$) sont statistiquement significatifs i.e. valeur p ($p<0.001$).
* Cependant, le coefficient de `baby.sleep` ($\hat{\beta}_2=0.011$) n'est pas statistiquement significatif.

`Conclusion`: La variable `baby.sleep` n'a pas d'effet significatif ; tout le travail est fait par la variable `dan.sleep`..

### Tableau d'analyse de la variance et test de signification de la régression du modèle {#15}
```{r }
# RLM avec tous les prédicteurs
anova(Full_model)
```
Le *test F* est utile pour vérifier la "performance du modèle dans son ensemble".

Dans ce cas, le modèle a une performance puisque le *F_stat* est de $215.2$ avec une valeur p $p < .001$. En outre, le $R^2 ajusté=0,812$ indique que le modèle de régression explique $81,2\%$ de la variabilité de la mesure des résultats, ce qui est bon.

## Régression linéaire avec le prédicteur "dan.sleep" {#3}


```{r }
# RLS avec le prédicteurs dan.sleep
Dan_model = lm(dan.grump ~ dan.sleep, data = data_parent) 
summary(Dan_model)     # affiche  coefficients
anova(Dan_model)      # Dans la RLS les statistiques t and F sont les mêmes.
```
Le modèle de régression linéaire simple est donc le suivant

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep**

* Le coef. ($\hat{\beta}_0=125.97$) et le coef. de `dan.sleep` ($\hat{\beta}_1=-8.9$) sont statistiquement significatifs, c'est-à-dire que la valeur p ($p<0.001$).
* De plus, le modèle a une performance significative, puisque le *F_stat* est de $434,9$ avec une valeur de p $p < .001$. De plus, le $R^2 ajusté=0,814$ indique que le modèle de régression explique $81,4\%$ de la variabilité de la mesure des résultats.

## Régression linéaire avec le prédicteur "baby.sleep". {#4}


```{r }
# RLS avec le prédicteurs baby.sleep
Baby_model = lm(dan.grump ~ baby.sleep, data = data_parent) 
summary(Baby_model)     # affiche coefficients
anova(Baby_model)      # Dans la RLS les statistiques t and F sont les mêmes.
```
Le modèle de régression linéaire simple est donc le suivant

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[3],3)`* **baby.sleep**

* Le coef. ($\hat{\beta}_0=125.97$) et le coef. de `baby.sleep` ($\hat{\beta}_2=-2.74$) sont statistiquement significatifs i.e. valeur p ($p<0.001$).
* De plus, le **modèle n'est  pas performant**, puisque le $R^2 ajusté=0,313$ indique que le modèle de régression explique $31,3\%$ de la variabilité de la mesure des résultats.

# Évaluation de la précision du modèle et la sélection du modèle {#5}
Le problème de la "sélection du modèle" est un problème relativement important. En d'autres termes, si nous disposons d'un ensemble de données contenant plusieurs variables, 

1. quelles sont celles que nous devons inclure en tant que variables prédictives;
2. quelles sont celles que nous ne devons pas inclure ? 

En d'autres termes, nous sommes confrontés à un problème de "sélection des variables". Tout d'abord, les deux principes à tenir en compte:

* Choisir un petit nombre de prédicteurs possibles qui présentent un intérêt théorique ;
* il y a un compromis entre la simplicité et la qualité de l'ajustement. Il faut éviter d'introduire trop de variables.

Rappelez-vous le principe de "rasoir d'Ockham" : "n'ajoutez pas un tas de prédicteurs non pertinents juste pour augmenter vos résultats".

## Sur la base de $R^2$ ajusté, RSE et Mean$_{se}$ {#6}

La précision du modèle peut être évaluée en examinant le "R-carré ajusté" $R^2$, l'"erreur quadratique moyenne" et l'"erreur type résiduelle (RSE)". 

Nous résumerons ces valeurs dans ce tableau.

| |Model complet | Model 1 (avec ban.sleep)| Model 2 (avec baby.sleep)
|:---------|:------------:|:------------:|:------------:
|	$R^2_{adj}$| **81.2**$\%$| **81.4**$\%$| $31.3\%$
| RSE |	**4.354**| **4.332**| 8.327
| EQM$_{se}$ |**8159.9** |**8159.9**|3202.7 

Le modèle complet et le modèle 1 (avec `dan.sleep` comme prédicteur) semblent avoir la même performance ($R^2_{adj}$ plus élevé et `MSE` plus bas) et sont meilleurs que le modèle 2 (avec `baby.sleep` comme prédicteur). Maintenant, lequel choisir ? 
**IMPORTANT**: plus le modèl est simple, mieux c'est. Par conséquent, je suppose que le modèle 1 est plus simple que le modèle complet. C'est-à-dire  

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep**

## Sur la base du critère d'information d'Akaike (AIC ; Akaike, 1974) {#7}

L'AIC pour un modèle qui a `K prédicteurs` plus une ordonnée à l'origine est :
$$
\mathrm{AIC}=\frac{\mathrm{SS}_{r e s}}{\hat{\sigma}^{2}}+2 K
$$
Plus la valeur AIC est petite, meilleure est la performance du modèle.

```{r }
AIC(Full_model,Dan_model)
```

Puisque l'AIC du modèle 1 (`dan.model`) est le plus bas, alors c'est le meilleur.

## basé sur Anova {#8}

teste l'hypothèse selon laquelle 

* $H_0$: model 1 (`dan_model`), prédicteurs sélectionnés.
* $H_1$: modèle complet (tous les prédicteurs)
```{r }
anova(Dan_model,Full_model)
```
Puisque la valeur p $p=0,969 > 0,05$, nous ne parvenons pas à rejeter l'hypothèse nulle, c'est-à-dire que c'est le "modèle 1 l'emporte".

Cette approche de la régression est souvent appelée "régression hiérarchique".

# Intervalles de confiance pour les paramètres {#9}

Rappelons que l'intervalle de confiance de $100(l - \alpha)$ pour le coefficient de régression $\beta_j,\;j = 0, 1, \cdots ,k$, est le suivant
$$
\hat{\beta}_{j}-t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} C_{j j}} \leq \beta_{j} \leq \hat{\beta}_{j}+t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} C_{i j}}
$$
où,

$C_{jj}$ est le $j$ ième élément de la diagonale de la matrice $(X'X)^{-1}$. Dans R, un IC à $95\%$ pour les paramètres est obtenu en utilisant **confint()**.

```{r }
confint(Full_model,level=0.95)
confint(Dan_model,level=0.95)
```
# Un IC 95% sur la moyenne lorsque le niveau de sommeil de Danielle est à 9,2 et 10,33 {#10}

Rappelons que l'intervalle de confiance en pourcentage de $100(l-\alpha)$ sur la réponse moyenne au point $\textbf{x}_0=(x_{01},x_{02},\cdots,x_{0k})'$ est de 

$$
\hat{y}_{0}-t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} \mathbf{x}_{0}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{x}_{0}} \leq E\left(y \mid x_{0}\right) \leq \hat{y}_{0}+t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} \mathbf{x}_{0}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{x}_{0}}
$$
où,

* $\textbf{x}_0=(x_{01},x_{02},\cdots,x_{0k})$ sont des nouvelles valeurs ;
* $(X'X)^{−1}$ est la matrice des covariables. Dans R, nous utilisons la fonction `predict` avec `interval="confidence:"`

```{r }
new <- data.frame(dan.sleep = c(9.2,10.33))  # crée de nouveles données

# cela permet d'obtenir un intervalle de confiance de 95 % sur la réponse moyenne
predict(Dan_model, new, interval="confidence")    
```
D'où, 

* l'IC de la réponse moyenne pour `dan.sleep=9.2' est $41.74\leq E\left(y \mid x_{0}\right) \leq 45.82$ ;
* l'IC de la réponse moyenne pour `dan.sleep=10.33` est $30.64\leq E\left(y \mid x_{0}\right) \leq 36.63$.


# Résumé

Dans cette présentation, nous avons abordé les sujets suivants 

* [Régression linéaire multiple](#1)
    a. [Régression linéaire multiple avec modèle complet](#2)
    c. [RL avec un prédicteur](#3) et [ici](#4)
* [Sélection du modèle](#5)
    a. [Sur la base de $R^2$, RSE et Mean$_{se}$ ajustés](#6)
    b. [Basé sur l'AIC](#7)
    c. [Basé sur Anova](#8)
* [Intervalles de confiance pour les paramètres](#9)
* [IC sur la réponse moyenne](#10)
* [Problème](#11)
  

# Problème à faire (p3.8 page 123) {#11}

Les données du tableau B.5 présentent la performance d'un procédé chimique en fonction de plusieurs variables contrôlables du procédé..

a. Ajuster un modèle de régression multiple reliant le produit $CO_2$ (y) au solvant total ($x_6$) et à la consommation d'hydrogène ($x_7$).
b. Testez la signification de la régression. Calculez $R^2$ et $R^2_{Adj}$.
c. A l'aide des tests t, déterminez la contribution de $x_6$ et $x_7$ au modèle.
d. Construire des IC à 95 % pour $\beta_6$ et $\beta_7$.
e. Reconstituer le modèle en utilisant uniquement $x_6$ comme régresseur. Testez la signification de la régression et calculez $R^2$ et $R^2_{Adj}$. Discutez de vos résultats. Sur la base de ces statistiques, êtes-vous satisfait de ce nouveau modèle ?
f. Construisez un IC à 95 % sur $\beta_6$ en utilisant le modèle que vous avez ajusté dans la question **e.** Comparez la longueur de cet IC à la longueur de l'IC dans la question **d.** Cela vous apprend-il quelque chose d'important sur la contribution de $x_7$ au modèle ?
g. Comparez les 2 modèles.

